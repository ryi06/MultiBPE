# Data generation for MultiBPE

## Generating data to train a MultiBPE model from scratch
Let's try to generate MultiBPE training data using ChIP-seq data from 5 conditions: [JUN.A549](https://www.encodeproject.org/experiments/ENCSR996DUT/), [JUNB.K562](https://www.encodeproject.org/experiments/ENCSR795IYP/), [JUND.A549](https://www.encodeproject.org/experiments/ENCSR000BRF/), [JUND.GM12878](https://www.encodeproject.org/experiments/ENCSR000DYS/) and [JUND.K562](https://www.encodeproject.org/experiments/ENCSR000EGN/).
Data generation for training a MultiBPE model requires the following metadata files: 
1. A text file specifying target TF ChIP-seq data locations.
2. A list of text files specifying locations of feature data files. 
3. A list of bed files specifying the genomic regions from which to generate train, validation and test examples.
4. A Python pickle file containing the indices for TF and cell type labels.

Example metadata files are provided in `data/metadata/training_example`.  The example pickle index file is provided in `data/embeddings`
```
data/metadata/training_example/
|-- ct_feature.txt
|-- target.txt
|-- test_regions.bed
|-- tf_feature.txt
|-- training_regions.bed
`-- validation_regions.bed
data/embeddings/
`-- example.pkl
```

For this demo, we include DNase-seq as our cell type specific feature, and HOCOMOCO TF motif enrichment as our TF specific features.

Make sure to also download the genome fasta file and save it in `data/annotations`:
```
mkdir data/annotations
cd data/annotations/
wget http://hgdownload.cse.ucsc.edu/goldenpath/hg19/bigZips/latest/hg19.fa.gz
gzip -d hg19.fa.gz
```
### Step 1: generate example sequences
`preprocess_v1-1_generate_example_sequences.sh` provides one way to generate example sequences from a group of TF ChIP-seq peaks. Combined peak set is first generated by concatenating multiple TF ChIP-seq peak bed files.  Peaks that are longer than `SEQUENCE_LENGTH` are removed. Two overlapping peaks are merged if
1. Two peaks overlap more than `MIN_OVERLAP` number of base pairs.
2. The merged peaks are shorter than `MAX_UNION` number of base pairs.

Each interval in the resulting merged peak set is used to create one example sequence of length `SEQUENCE_LENGTH` where the midpoints of the example and the interval are the same.

The above procedure can be achieved by running the following:
```
cd preprocess
./preprocess_v1-1_generate_example_sequences.sh \
--region_bed ../data/metadata/training_example/training_regions.bed,../data/metadata/training_example/validation_regions.bed,../data/metadata/training_example/test_regions.bed \
--output_prefix training,validation,test \
--output_dir ../data/datasets/training_example
```
The main goal of this step is to create an example `.pkl` file necessary for the downstream data collection steps. It specifies the genomic locations of each example sequence, and where the retrieved feature signal tracks and target labels will be saved.

Alternatively, you can provide your own peak set stored in the [bed file format](https://genome.ucsc.edu/FAQ/FAQformat.html#format1) and use `preprocess_v1-1-2_generate_custom_examples.sh` to generate the example `.pkl` file. See example [here](#generate-example-sequences).


### Step 2: retrieve feature signal tracks and target labels
For each dataset (training, validation and test), we need to retrieve feature signals (DNase-seq signal and HOCOMOCO motif enrichment) as well as target labels (from conserved and relaxed peak sets). `preprocess_v1-2_retrieve_signal.sh` retrieves necessary data and performs zscore normalization on the retrieved feature signals. For small datasets, we can do this sequentially by running the following: 
```
declare -a datasets=("training" "validation" "test")
for dset in "${datasets[@]}"; do
	EXAMPLE_PICKLE="../data/datasets/training_example/${dset}/${dset}_minOverlap200_maxUnion600_example.pkl"
	REGION_BED="../data/metadata/training_example/${dset}_regions.bed"
	ZSCORE_DIR="../data/datasets/training_example/${dset}/zscore_params"
	# Retrieve target labels
	# Process 2nd to 6th entries in ../data/metadata/training_example/target.txt
	for job_id in {2..6}; do 
		# Conserved peaks
		./preprocess_v1-2_retrieve_signal.sh --job_id $job_id \
		--example_pickle $EXAMPLE_PICKLE --region_bed $REGION_BED
		# Relaxed peaks
		./preprocess_v1-2_retrieve_signal.sh --job_id $job_id \
		--example_pickle $EXAMPLE_PICKLE --region_bed $REGION_BED \
		--target_type "output_relaxed"
	done

	# Retrieve DNase signal
	# Process 2nd to 4th entries in ../data/metadata/training_example/ct_feature.txt
	for job_id in {2..4}; do
		./preprocess_v1-2_retrieve_signal.sh --job_id $job_id \
		--metadata "../data/metadata/training_example/ct_feature.txt" \
		--example_pickle $EXAMPLE_PICKLE --region_bed $REGION_BED \
		--zscore_dir $ZSCORE_DIR
	done

	# Retrieve hocomoco motif enrichment scores
	# Process 2nd to 4th entries in ../data/metadata/training_example/tf_feature.txt
	for job_id in {2..4}; do
		./preprocess_v1-2_retrieve_signal.sh --job_id $job_id \
		--metadata "../data/metadata/training_example/tf_feature.txt" \
		--example_pickle $EXAMPLE_PICKLE --region_bed $REGION_BED \
		--zscore_dir $ZSCORE_DIR
	done
done
```
I recommend running this step on a high performance computing environment if you have a large number of example sequences and conditions to get data from. If you're working under [Slurm](https://slurm.schedmd.com/), you can submit array jobs similar to this:
```
declare -a datasets=("training" "validation" "test")
for dset in "${datasets[@]}"; do
	EXAMPLE_PICKLE="../data/datasets/training_example/${dset}/${dset}_minOverlap200_maxUnion600_example.pkl"
	REGION_BED="../data/metadata/training_example/${dset}_regions.bed"
	ZSCORE_DIR="../data/datasets/training_example/${dset}/zscore_params"
	# Retrieve target labels
	# Process 2nd to 6th entries in ../data/metadata/training_example/target.txt
	sbatch --array=2-6 preprocess_v1-2_retrieve_signal.sh \
	--example_pickle $EXAMPLE_PICKLE --region_bed $REGION_BED \
	--tmp_dir $SLURM_JOBTMP
	sbatch --array=2-6 preprocess_v1-2_retrieve_signal.sh \
	--example_pickle $EXAMPLE_PICKLE --region_bed $REGION_BED \
	--target_type "output_relaxed" --tmp_dir $SLURM_JOBTMP

	# Retrieve DNase signal
	# Process 2nd to 4th entries in ../data/metadata/training_example/ct_feature.txt
	sbatch --array=2-4 preprocess_v1-2_retrieve_signal.sh \
	--metadata "../data/metadata/training_example/ct_feature.txt" \
	--example_pickle $EXAMPLE_PICKLE --region_bed $REGION_BED \
	--zscore_dir $ZSCORE_DIR --tmp_dir $SLURM_JOBTMP

	# Retrieve hocomoco motif enrichment scores
	# Process 2nd to 4th entries in ../data/metadata/training_example/tf_feature.txt
	sbatch --array=2-4 preprocess_v1-2_retrieve_signal.sh \
	--metadata "../data/metadata/training_example/tf_feature.txt" \
	--example_pickle $EXAMPLE_PICKLE --region_bed $REGION_BED \
	--zscore_dir $ZSCORE_DIR --tmp_dir $SLURM_JOBTMP
done
``` 
### Step 3: Merge data files
The last step is to combine the retrieved feature signals as well as target labels into one [HDF5 file](https://www.hdfgroup.org/solutions/hdf5/), and compute class weight in target labels. 
```
declare -a datasets=("training" "validation" "test")
for dset in "${datasets[@]}"; do
	EXAMPLE_PICKLE="../data/datasets/training_example/${dset}/${dset}_minOverlap200_maxUnion600_example.pkl"
	OUTPUT_HDF5="../data/datasets/training_example/${dset}_minOverlap200_maxUnion600_example.h5"
	./preprocess_v1-3_merge_dataset.sh --example_pickle $EXAMPLE_PICKLE \
	--output_hdf5 $OUTPUT_HDF5
done
```
[Here](../data/datasets/training_example/training_example_directory_structure.md) is a list of files you will be able to generate after successfully running the above three steps.

## Generating data to make predictions from a trained model
In this demo, we use the [pretrained model](../pretrained/seq_CT/) to make predictions from 2 conditions: [JUN.K562](https://www.encodeproject.org/experiments/ENCSR000EFS/) and [JUNB.GM12878](https://www.encodeproject.org/experiments/ENCSR897MMC/). This particular model was trained on the DNA sequence as well as 4 types of cell type-specific features (DNase, H3K4me1, H3K4me3, H3K27ac). 

Data generation for MultiBPE prediction requires the following metadata files: 
1. A text file specifying the conditions to make predictions for.
2. A list of text files specifying locations of feature data files. 
3.  A bed file specifying the genomic regions from which to generate prediction examples.
4. A Python pickle file containing the indices for TF and cell type labels.

Example metadata files are provided in `data/metadata/prediction_example`.  The example pickle index file is provided in `data/embeddings`. 
```
data/metadata/prediction_example/
|-- conditions.txt
|-- ct_feature.txt
|-- predict.bed
`-- tf_feature.txt
data/embeddings/
`-- pretrained.pkl
```

### Generate example sequences
Generate example sequences given a peak set bed file. Each interval in the bed file is used to create one example sequence of length `SEQUENCE_LENGTH` where the midpoints of the example and the interval are the same. If intervals in your bed file are much longer than `SEQUENCE_LENGTH`, we recommend binning the intervals before running this script. Turn on the `--skip_target` flag when target labels are not available.
```
cd preprocess
./preprocess_v1-1-2_generate_custom_examples.sh \
--input_bed ../data/metadata/prediction_example/predict.bed \
--output_dir ../data/datasets/prediction_example \
--set_path_extra_args --ct_feature,DNase,H3K4me1,H3K4me3,H3K27ac,--skip_target
```

### Retrieve feature signal tracks
Retreive feature signals (DNase-seq and HOCOMOCO motif enrichment) for prediction dataset. Run the following to retrieve data sequentially. See [similar example](#step-2-retrieve-feature-signal-tracks-and-target-labels) above for instructions to retrieve data in a high performance computing environment such as [Slurm](https://slurm.schedmd.com/).
```
# Process 2nd and 9th entries in ../data/metadata/prediction_example/ct_feature.txt
for job_id in {2..9}; do
	./preprocess_v1-2_retrieve_signal.sh --job_id $job_id \
	--metadata "../data/metadata/prediction_example/ct_feature.txt" \
	--example_pickle "../data/datasets/prediction_example/predict_example.pkl" \
	--zscore_dir "../data/datasets/prediction_example/zscore_params"
done
```

### Merge data files
Combine the retrieved feature signals into one [HDF5 file](https://www.hdfgroup.org/solutions/hdf5/). Turn on the `--skip_target` flag when target labels are not available.
```
./preprocess_v1-3_merge_dataset.sh \
--example_pickle "../data/datasets/prediction_example/predict_example.pkl" \
--output_hdf5 "../data/datasets/prediction_example/predict_example.h5" \
--generate_hdf5_extra_args "--ct_feature,DNase,H3K4me1,H3K4me3,H3K27ac,--compression,--skip_target,--condition_metadata,../data/metadata/prediction_example/conditions.txt"
```

[Here](../data/datasets/prediction_example/prediction_example_directory_structure.md) is a list of files you will be able to generate after successfully running the above three steps.
<!--stackedit_data:
eyJoaXN0b3J5IjpbMzQ1ODAxNDI3LC0zMjAyNDY4NDddfQ==
-->