# Generating training and prediction data for MultiBPE

## Data generation for training
Let's try to generate MultiBPE training data using ChIP-seq data from 5 conditions: [JUN.A549](https://www.encodeproject.org/experiments/ENCSR996DUT/), [JUNB.K562](https://www.encodeproject.org/experiments/ENCSR795IYP/), [JUND.A549](https://www.encodeproject.org/experiments/ENCSR000BRF/), [JUND.GM12878](https://www.encodeproject.org/experiments/ENCSR000DYS/) and [JUND.K562](https://www.encodeproject.org/experiments/ENCSR000EGN/).
Data generation for training a MultiBPE model requires the following metadata files: 
1. A text file specifying target TF ChIP-seq data locations.
2. A list of text files specifying locations of feature data files. 
3. A list of bed files specifying the genomic regions from which to generate train, validation and test examples.
4. A Python pickle file containing the indices for TF and cell type labels.

Example metadata files are provided in `data/metadata/training_example`.  The example pickle index file is provided in `data/embeddings`
```
data/metadata/training_example/
|-- in_ENCODE_DNase.txt
|-- in_ENCODE_hocomoco.txt
|-- out_ENCODE_TF_ChIP.txt
|-- test_regions.bed
|-- train_regions.bed
`-- validation_regions.bed
data/embeddings/
`-- example.pkl
```

For this demo, we include DNase-seq as our cell type specific feature, and HOCOMOCO TF motif enrichment as our TF specific features. More features can be included by providing additional metadata text files.

Make sure to also download the genome fasta file and save it in `data/annotations`:
```
mkdir data/annotations
cd data/annotations/
wget http://hgdownload.cse.ucsc.edu/goldenpath/hg19/bigZips/latest/hg19.fa.gz
gzip -d hg19.fa.gz
```
### Step 1: generate example sequences
`preprocess_v1-1_generate_example_sequences.sh` provides one way to generate example sequences from a group of TF ChIP-seq peaks. Combined peak set is first generated by concatenating multiple TF ChIP-seq peak bed files.  Peaks that are longer than `SEQUENCE_LENGTH` are removed. Two overlapping peaks are merged if
1. Two peaks overlap more than `MIN_OVERLAP` number of base pairs.
2. The merged peaks are shorter than `MAX_UNION` number of base pairs.

Each interval in the resulting merged peak set is used to create one example sequence of length `SEQUENCE_LENGTH` where the midpoints of the example and the interval are the same.

The above procedure can be achieved by running the following:
```
cd preprocess
./preprocess_v1-1_generate_example_sequences.sh \
--region_bed ../data/metadata/training_example/train_regions.bed,../data/metadata/training_example/validation_regions.bed,../data/metadata/training_example/test_regions.bed --output_prefix train,validation,test \
--output_dir ../data/datasets/training_example
```
The main goal of this step is to create an example `.pkl` file necessary for the downstream data collection steps. It specifies the genomic locations of each example sequence, and where the retrieved feature signal track and target labels will be saved.

Alternatively, you can provide your own peak set stored in the [bed file format](https://genome.ucsc.edu/FAQ/FAQformat.html#format1) and use `preprocess_v1-1-2_generate_custom_examples.sh` to generate the example `.pkl` file. See example [here](#generate-example-sequences).


### Step 2: retrieve feature signal tracks and target labels
For each dataset (train, validation and test), we need to retrieve feature signals (DNase-seq signal and HOCOMOCO motif enrichment) as well as target labels (from conserved and relaxed peak sets). `preprocess_v1-2_retrieve_signal.sh` retrieves necessary data and performs zscore normalization on the retrieved feature signals. For small datasets, we can do this sequentially by running the following: 
```
declare -a datasets=("train" "validation" "test")
for dset in "${datasets[@]}"; do
	EXAMPLE_PICKLE="../data/datasets/training_example/${dset}/${dset}_minOverlap200_maxUnion600_example.pkl"
	REGION_BED="../data/metadata/training_example/${dset}_regions.bed"
	ZSCORE_DIR="../data/datasets/training_example/${dset}/zscore_params"
	# Retrieve target labels
	# Process 2nd to 6th entries in ../data/metadata/training_example/out_ENCODE_TF_ChIP.txt
	for job_id in {2..6}; do 
		# Conserved peaks
		./preprocess_v1-2_retrieve_signal.sh --job_id $job_id \
		--example_pickle $EXAMPLE_PICKLE --region_bed $REGION_BED
		# Relaxed peaks
		./preprocess_v1-2_retrieve_signal.sh --job_id $job_id \
		--example_pickle $EXAMPLE_PICKLE --region_bed $REGION_BED \
		--target_type "output_relaxed"
	done

	# Retrieve DNase signal
	# Process 2nd to 4th entries in ../data/metadata/training_example/in_ENCODE_DNase.txt
	for job_id in {2..4}; do
		./preprocess_v1-2_retrieve_signal.sh --job_id $job_id \
		--metadata "../data/metadata/training_example/in_ENCODE_DNase.txt" \
		--example_pickle $EXAMPLE_PICKLE --region_bed $REGION_BED \
		--zscore_dir $ZSCORE_DIR
	done

	# Retrieve hocomoco motif enrichment scores
	# Process 2nd to 4th entries in ../data/metadata/training_example/in_ENCODE_hocomoco.txt
	for job_id in {2..4}; do
		./preprocess_v1-2_retrieve_signal.sh --job_id $job_id \
		--metadata "../data/metadata/training_example/in_ENCODE_hocomoco.txt" \
		--example_pickle $EXAMPLE_PICKLE --region_bed $REGION_BED \
		--zscore_dir $ZSCORE_DIR
	done
done
```
I recommend running this step on a high performance computing environment if you have a large number of example sequences and conditions to get data from. If you're working under [Slurm](https://slurm.schedmd.com/), you can submit array jobs similar to this:
```
declare -a datasets=("train" "validation" "test")
for dset in "${datasets[@]}"; do
	EXAMPLE_PICKLE="../data/datasets/training_example/${dset}/${dset}_minOverlap200_maxUnion600_example.pkl"
	REGION_BED="../data/metadata/training_example/${dset}_regions.bed"
	ZSCORE_DIR="../data/datasets/training_example/${dset}/zscore_params"
	# Retrieve target labels
	# Process 2nd to 6th entries in ../data/metadata/training_example/out_ENCODE_TF_ChIP.txt
	sbatch --array=2-6 preprocess_v1-2_retrieve_signal.sh \
	--example_pickle $EXAMPLE_PICKLE --region_bed $REGION_BED \
	--tmp_dir $SLURM_JOBTMP
	sbatch --array=2-6 preprocess_v1-2_retrieve_signal.sh \
	--example_pickle $EXAMPLE_PICKLE --region_bed $REGION_BED \
	--target_type "output_relaxed" --tmp_dir $SLURM_JOBTMP

	# Retrieve DNase signal
	# Process 2nd to 4th entries in ../data/metadata/training_example/in_ENCODE_DNase.txt
	sbatch --array=2-4 preprocess_v1-2_retrieve_signal.sh \
	--metadata "../data/metadata/training_example/in_ENCODE_DNase.txt" \
	--example_pickle $EXAMPLE_PICKLE --region_bed $REGION_BED \
	--zscore_dir $ZSCORE_DIR --tmp_dir $SLURM_JOBTMP

	# Retrieve hocomoco motif enrichment scores
	# Process 2nd to 4th entries in ../data/metadata/training_example/in_ENCODE_hocomoco.txt
	sbatch --array=2-4 preprocess_v1-2_retrieve_signal.sh \
	--metadata "../data/metadata/training_example/in_ENCODE_hocomoco.txt" \
	--example_pickle $EXAMPLE_PICKLE --region_bed $REGION_BED \
	--zscore_dir $ZSCORE_DIR --tmp_dir $SLURM_JOBTMP
done
``` 
### Step 3: generate hdf5 data files
The last step is to combine the retrieved feature signals as well as target labels into one [HDF5 file](https://www.hdfgroup.org/solutions/hdf5/).
```
declare -a datasets=("train" "validation" "test")
for dset in "${datasets[@]}"; do
	EXAMPLE_PICKLE="../data/datasets/training_example/${dset}/${dset}_minOverlap200_maxUnion600_example.pkl"
	OUTPUT_HDF5="../data/datasets/training_example/${dset}_minOverlap200_maxUnion600_example.h5"
	./preprocess_v1-3_generate_hdf5.sh --example_pickle $EXAMPLE_PICKLE \
	--output_hdf5 $OUTPUT_HDF5
done
```
[Here](../data/datasets/training_example/training_example_directory_structure.md) is a list of files you will be able to generate after successfully running the above three steps.


## Data generation for prediction
Let's use the trained model to make predictions using ChIP-seq data from 2 conditions: [JUN.K562](https://www.encodeproject.org/experiments/ENCSR000EFS/) and [JUNB.GM12878](https://www.encodeproject.org/experiments/ENCSR897MMC/).
Data generation for MultiBPE prediction requires the following metadata files: 
1. A text file specifying the conditions to make predictions for.
2. A list of text files specifying locations of feature data files. 
3.  A bed file specifying the genomic regions from which to generate prediction examples.
4. A Python pickle file containing the indices for TF and cell type labels.

Example metadata files are provided in `data/metadata/prediction_example`.  The example pickle index file is provided in `data/embeddings`. 
```
data/metadata/prediction_example/
|-- conditions.txt
|-- in_ENCODE_DNase.txt
|-- in_ENCODE_hocomoco.txt
`-- predict.bed
data/embeddings/
`-- example.pkl
```

### Generate example sequences
Generate example sequences given a peak set bed file. Each interval in the bed file is used to create one example sequence of length `SEQUENCE_LENGTH` where the midpoints of the example and the interval are the same. If intervals in your bed file are much longer than `SEQUENCE_LENGTH`, we recommend binning the intervals first before running this script. Turn on the `--skip_target` flag when target labels are not available.
```
cd preprocess
./preprocess_v1-1-2_generate_custom_examples.sh \
--input_bed ../data/metadata/prediction_example/predict.bed \
--output_dir ../data/datasets/prediction_example \
--set_path_extra_args --ct_feature,DNase,--tf_feature,hocomoco,--skip_target
```

### Retrieve feature signal tracks
Retreive feature signals (DNase-seq and HOCOMOCO motif enrichment) for prediction dataset. Run the following to retrieve data sequentially. See [similar example above](#step-2-retrieve-feature-signal-tracks-and-target-labels) for instructions to retrieve data in a high performance computing environment such as [Slurm](https://slurm.schedmd.com/).
```
EXAMPLE_PICKLE="../data/datasets/prediction_example/predict_example.pkl"
ZSCORE_DIR="../data/datasets/prediction_example/zscore_params"

# Retrieve DNase signal
# Process 2nd and 3rd entries in ../data/metadata/prediction_example/in_ENCODE_DNase.txt
for job_id in {2..3}; do
	./preprocess_v1-2_retrieve_signal.sh --job_id $job_id \
	--metadata "../data/metadata/prediction_example/in_ENCODE_DNase.txt" \
	--example_pickle $EXAMPLE_PICKLE --zscore_dir $ZSCORE_DIR
done

# Retrieve hocomoco motif enrichment scores
# Process 2nd and 3rd entries in ../data/metadata/prediction_example/in_ENCODE_hocomoco.txt
for job_id in {2..3}; do
	./preprocess_v1-2_retrieve_signal.sh --job_id $job_id \
	--metadata "../data/metadata/prediction_example/in_ENCODE_hocomoco.txt" \
	--example_pickle $EXAMPLE_PICKLE --zscore_dir $ZSCORE_DIR
done
```

### Generate hdf5 data file
Combine the retrieved feature signals into one [HDF5 file](https://www.hdfgroup.org/solutions/hdf5/). Turn on the `--skip_target` flag when target labels are not available.
```
EXAMPLE_PICKLE="../data/datasets/prediction_example/predict_example.pkl"
OUTPUT_HDF5="../data/datasets/prediction_example/predict_example.h5"
EXTRA_ARGS="--ct_feature,DNase,--tf_feature,hocomoco,--compression,--skip_target,--condition_metadata,../data/metadata/prediction_example/conditions.txt"
./preprocess_v1-3_generate_hdf5.sh --example_pickle $EXAMPLE_PICKLE \
--output_hdf5 $OUTPUT_HDF5 --generate_hdf5_extra_args $EXTRA_ARGS
```

[Here](../data/datasets/prediction_example/prediction_example_directory_structure.md) is a list of files you will be able to generate after successfully running the above three steps.
<!--stackedit_data:
eyJoaXN0b3J5IjpbNzAyMzM2ODI1LDc4MjcxMzgxMywtMTY5Nz
M5ODYzMSw3OTk3OTc3OTksODk0Mjg3NzAxLC0xMzg0MzE4MTAs
OTk5NTA4NTA4LDIwNDYwNTU1MzEsMTY0MDkwODM1OSw4NDQyOT
E5MzEsMzc0MjM3MTI0LDEyMDk4ODgxMCwxMzM3ODEyMTg4LDY5
MTMwNDc2OSwxMjc3NjgzMjcsMjY2NzUxOTQ4LC0yMTI0NTUyNz
EzLC0xMjUxNzgxNTU3LC05MjUzNTA3ODIsLTE5NjA5MzU1OTBd
fQ==
-->